{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Question 4]  Leveraging a Pretrained Model for CIFAR-10 (10 marks)**\n",
    "\n",
    "** optimizer2=tf.keras.optimizers.SGD(learning_rate=0.0001, momentum=0.9) undecided how to use this or insert where **\n",
    "\n",
    "> Objective: Use a pretrained model from TensorFlow's Keras applications  **as a feature extractor or for fine-tuning on the CIFAR-10 dataset**, and a**chieve >90% accuracy on the test dataset** of CIFAR-10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Z5yylhUil28q"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#import the libraries and packages needed\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m models, layers, optimizers\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "#import the libraries and packages needed\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers, optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Requirement**: Dataset Preparation: Load the CIFAR-10 dataset, applying normalization and any required preprocessing to match the input format of the pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ix0vBFavl48F"
   },
   "outputs": [],
   "source": [
    "def preprocess_data(X, Y, num_class=10):\n",
    "    \"\"\"\n",
    "    returns preprocessed data\n",
    "    by using the resnet50 preprocess_input function which normalizes the input data and\n",
    "    converts the labels to one-hot encoding using the to_categorical function with 10 classes\n",
    "    \"\"\"\n",
    "    #X_resized = tf.image.resize(X, (224, 224))\n",
    "    X_p = tf.keras.applications.resnet50.preprocess_input(X)\n",
    "    Y_p = tf.keras.utils.to_categorical(Y, num_class)\n",
    "    return X_p, Y_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4HbqkrWfmO3c",
    "outputId": "953e6c87-a692-4007-b13f-a2c755e5fab7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170498071/170498071 [==============================] - 13s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Load the CIFAR-10 dataset\n",
    "(X_train, Y_train), (X_test, Y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# preprocess data using the preprocess_data function\n",
    "#''''\n",
    "X_train_p, Y_train_p = preprocess_data(X_train, Y_train)\n",
    "X_test_p, Y_test_p = preprocess_data(X_test, Y_test)\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ocU-Ae-GmQND",
    "outputId": "7208a3cb-28e2-4ccc-da5b-545e7de097e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3) (50000, 10)\n",
      "(10000, 32, 32, 3) (10000, 10)\n",
      "number of classes: 10\n",
      "first 5 labels:\n",
      " [[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] \n",
      "\n",
      "max, and min of the datapoints: 151.061 -123.68\n"
     ]
    }
   ],
   "source": [
    "# sanity check of the shape of the preprocessed data, datapoints statistics,\n",
    "# and labels if its turned into one-hot encoding\n",
    "print(X_train_p.shape, Y_train_p.shape)\n",
    "print(X_test_p.shape, Y_test_p.shape)\n",
    "print('number of classes:',num_class:=Y_train_p.shape[1])\n",
    "print('first 5 labels:\\n', Y_train_p[:5],'\\n')\n",
    "print('max, and min of the datapoints:', X_train_p.max(), X_train_p.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Requirement**: Model Selection and Modification: Choose a pretrained model (e.g., ResNet, VGG16) and modify it for CIFAR-10 classification. Describe how you adapt the model for the 10 classes of CIFAR-10 (e.g., modifying the top layer, adjusting input size)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To use ResNet50 pretrained on ImageNet dataset as a feature extractor\n",
    "- To **remove** the final layer of ResNet50 that is used to classify for the ImageNet data\n",
    "- To add a dense layer with 10 classes with softmax activations because the original classifier has 1000 classes (from ImageNet)\n",
    "- To freeze all the prior layers of ResNet50, and perform transfer learning on the layer(s) added to adapt to learning how to classify the CIFAR10 images \n",
    "- Stacking: Add upscaling of CIFAR10 images (32x32x3) to match the spatial dimension of ResNet50 (224x224x3) as it was trained on ImageNet 224x224x3 images -> ResNet50 as base model -> added layer(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FctWxZa5mZi1",
    "outputId": "59da45c9-3761-40f7-e578-c2a15eec79e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94765736/94765736 [==============================] - 5s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Load ResNet50 model without the top layer\n",
    "base_model = tf.keras.applications.ResNet50(include_top=False, \n",
    "                                            weights='imagenet', \n",
    "                                            input_shape=(224, 224, 3))\n",
    "base_model.trainable = False  # Freeze ResNet50 layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Model 1 architecture**: Upscaling -> Base Model (ResNet50) -> Flatten -> Dense layer with Softmax Activation (Classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model1\n",
    "model1 = models.Sequential([\n",
    "    # Upscale images\n",
    "    layers.experimental.preprocessing.Resizing(224, 224, interpolation=\"bilinear\"),\n",
    "    # Add the pre-trained ResNet50 base model\n",
    "    base_model,\n",
    "    # Add custom layers on top\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(10, activation='softmax')  # For CIFAR-10 classes\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model1.compile(optimizer=optimizers.SGD(lr=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Describe the model architecture and params for sanity check\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(X_train, Y_train_p, batch_size=128, epochs=5,\n",
    "          validation_data=(X_test, Y_test_p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Model1 Evaluation**: Observed that the model is overfitting quite badly from epoch 3 onwards as the gap between train acc and test acc is widening. Upon research, decided to use Global Average Pooling (GAP)to find the precense of a global feature across all 3 channels, instead of purely just flattening before feeding it into the classifier (softmax). As a result of reducing the number of parameters, this can help prevent overfitting\n",
    "\n",
    "> Reference: https://stackabuse.com/dont-use-flatten-global-pooling-for-cnns-with-tensorflow-and-keras/ , https://paperswithcode.com/method/global-average-pooling#:~:text=Global%20Average%20Pooling%20is%20a%20pooling%20operation%20designed,the%20classification%20task%20in%20the%20last%20mlpconv%20layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Model2 Architecture**: Upscaling -> Base Model (ResNet50) -> Global Average Pooling -> Dense layer with Softmax Activation (Classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model2 = models.Sequential([\n",
    "    # Upscale images\n",
    "    layers.experimental.preprocessing.Resizing(224, 224, interpolation=\"bilinear\"),\n",
    "    # Add the pre-trained ResNet50 base model\n",
    "    base_model,\n",
    "    # Add custom layers on top\n",
    "    layers.Global Average Pooling,\n",
    "    layers.Dense(10, activation='softmax')  # For CIFAR-10 classes\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model2.compile(optimizer=optimizers.SGD(lr=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# will just start small with 5 epochs\n",
    "model2.fit(X_train, Y_train_p, batch_size=128, epochs=5,\n",
    "          validation_data=(X_test, Y_test_p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Model 2 evauation** : Improved performance with using GAP but unable to hit 90%. Decided to increase a few more hidden layers with batch norm and softmax activation like what we did on CNN. Batch norm is added and applied immediately after the dense layer to increases training stability and improves convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Model 3 architecture**: Upscaling > Base Model (Resnet50) > Dense layer of 512 neurons -> Batch normalisation -> Relu activation -> Dense layer of 128 neurons -> Batch normalisation -> Relu activation -> Dense layer with Softmax Activation (Classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "zwPywf8PmbHI"
   },
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = models.Sequential([\n",
    "    # Upscale images\n",
    "    layers.experimental.preprocessing.Resizing(224, 224, interpolation=\"bilinear\"),\n",
    "    # Add the pre-trained ResNet50 base model\n",
    "    base_model,\n",
    "    # Add custom layers on top\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(10, activation='softmax')  # For CIFAR-10 classes\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Kr4KZGMmduo",
    "outputId": "f1af553e-3096-45ed-f048-024c6e22fef6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "391/391 [==============================] - 194s 464ms/step - loss: 0.9557 - accuracy: 0.7312 - val_loss: 0.4907 - val_accuracy: 0.8352\n",
      "Epoch 2/5\n",
      "391/391 [==============================] - 184s 471ms/step - loss: 0.4359 - accuracy: 0.8529 - val_loss: 0.4059 - val_accuracy: 0.8607\n",
      "Epoch 3/5\n",
      "391/391 [==============================] - 187s 480ms/step - loss: 0.3198 - accuracy: 0.8914 - val_loss: 0.3880 - val_accuracy: 0.8713\n",
      "Epoch 4/5\n",
      "391/391 [==============================] - 188s 482ms/step - loss: 0.2382 - accuracy: 0.9182 - val_loss: 0.3985 - val_accuracy: 0.8717\n",
      "Epoch 5/5\n",
      "391/391 [==============================] - 192s 492ms/step - loss: 0.1844 - accuracy: 0.9374 - val_loss: 0.3987 - val_accuracy: 0.8782\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7e50e0171d20>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=optimizers.SGD(lr=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Train the model\n",
    "# will just start small with 5 epochs\n",
    "model.fit(X_train, Y_train_p, batch_size=128, epochs=5,\n",
    "          validation_data=(X_test, Y_test_p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*stop* here ------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "0WWvlON8mkYt"
   },
   "outputs": [],
   "source": [
    "# Build the model2; improved version by adding batch normalization to adjust the activations with a mean of 0 and std dev of 1\n",
    "# batch norm added AFTER activations. source:https://www.deeplearningbook.org/contents/optimization.html\n",
    "model2 = models.Sequential([\n",
    "    # Upscale images\n",
    "    layers.experimental.preprocessing.Resizing(224, 224, interpolation=\"bilinear\"),\n",
    "    base_model,\n",
    "    layers.BatchNormalization(),  # Add Batch Normalization after ResNet50\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.BatchNormalization(),  # Add Batch Normalization after the first Dense layer\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.BatchNormalization(),  # Add Batch Normalization after the second Dense layer\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(10, activation='softmax')  # Output layer for CIFAR-10 classes\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2f7Ie10hrh--",
    "outputId": "69b3ec13-3a51-4ba6-b46f-29d0408ec091"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "391/391 [==============================] - 204s 506ms/step - loss: 0.7420 - accuracy: 0.7579 - val_loss: 0.3960 - val_accuracy: 0.8698\n",
      "Epoch 2/5\n",
      "391/391 [==============================] - 182s 466ms/step - loss: 0.3616 - accuracy: 0.8826 - val_loss: 0.3480 - val_accuracy: 0.8865\n",
      "Epoch 3/5\n",
      "391/391 [==============================] - 195s 499ms/step - loss: 0.2818 - accuracy: 0.9105 - val_loss: 0.3231 - val_accuracy: 0.8954\n",
      "Epoch 4/5\n",
      "391/391 [==============================] - 184s 471ms/step - loss: 0.2285 - accuracy: 0.9307 - val_loss: 0.3087 - val_accuracy: 0.8991\n",
      "Epoch 5/5\n",
      "391/391 [==============================] - 195s 499ms/step - loss: 0.1915 - accuracy: 0.9433 - val_loss: 0.2990 - val_accuracy: 0.9015\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7e505025db40>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile the model\n",
    "# for SGD, learning rate decay is used to gradually reduces the learning rate during training,\n",
    "# allowing the model to fine-tune and adjust weights with greater precision as it approaches convergence\n",
    "optimizer2=tf.keras.optimizers.SGD(learning_rate=0.0001, momentum=0.9)\n",
    "model2.compile(optimizer=optimizer2,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model2.fit(X_train_p, Y_train_p, batch_size=128, epochs=5,\n",
    "          validation_data=(X_test_p, Y_test_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ZSRbk6YsP9e"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
